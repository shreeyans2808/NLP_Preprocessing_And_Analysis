{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization using Spacy(library for nlp)"
      ],
      "metadata": {
        "id": "pyITwbDnQEe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this or Auto_Tokenizer from huggingface transformers library with bert tokenizer to get tokens. (these two are considered the best)."
      ],
      "metadata": {
        "id": "-ow2jIarR1NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "-0cauiYoQPTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string=\"Do you think we will be able to attend the meeting that was happening at night for the fun of it?\""
      ],
      "metadata": {
        "id": "pLvUElCBQgx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(string)\n",
        "words=[]\n",
        "for token in doc:\n",
        "    print(token.text,end=\"|\")\n",
        "    words.append(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKh9J7F7QfiF",
        "outputId": "963538a1-5eb1-4561-b833-c07e2dfc3da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do|you|think|we|will|be|able|to|attend|the|meeting|that|was|happening|at|night|for|the|fun|of|it|?|"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbtTtJolRpZn",
        "outputId": "55564b23-669b-46ac-d310-ac572a50d400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chut"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taa3nn8bRhRQ",
        "outputId": "ffea617f-4ac9-4624-bfad-c139c9b8cf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tum',\n",
              " 'Dono',\n",
              " 'Ki',\n",
              " 'Maa',\n",
              " 'Ki',\n",
              " 'Chut',\n",
              " '.',\n",
              " 'Kya',\n",
              " 'Bolti',\n",
              " 'Puplic',\n",
              " \"'s\",\n",
              " 'Hai',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc6i--WyR-A8",
        "outputId": "a0cce73a-3b63-4c73-ef96-e6c04badca03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tum Dono Ki Maa Ki Chut\n",
            "Kya Bolti Puplic's Hai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also use re library for multiple functions, mainly to replace certain words\n",
        "(to be done before tokeinsing)"
      ],
      "metadata": {
        "id": "ofd34r2KTQeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lematization(helps making nlp understand better - (like for help, helping for both , help is the right word))"
      ],
      "metadata": {
        "id": "dep-L5PHVTnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text,token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8sJ-X2FVSSG",
        "outputId": "364057e4-214a-46fd-960d-dee94fadc763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do do\n",
            "you you\n",
            "think think\n",
            "we we\n",
            "will will\n",
            "be be\n",
            "able able\n",
            "to to\n",
            "attend attend\n",
            "the the\n",
            "meeting meeting\n",
            "that that\n",
            "was be\n",
            "happening happen\n",
            "at at\n",
            "night night\n",
            "for for\n",
            "the the\n",
            "fun fun\n",
            "of of\n",
            "it it\n",
            "? ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopwords words that do not contribute anything to the tsatemnt( like and, the), this is to be used when making analysis sytems"
      ],
      "metadata": {
        "id": "ob-YG_opW7gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "lkKkZdtOXCXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYxWnvYYNPw",
        "outputId": "14ce40b1-a79b-45cf-9af0-9e71b2bb97e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWH3wzHHYuI3",
        "outputId": "86c89c4d-f6e2-40f7-a798-dafce3f05724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtpvSHvSYx2k",
        "outputId": "6c17a516-34ea-4366-f6cf-5099a378f8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk import word_tokenize,sent_tokenize"
      ],
      "metadata": {
        "id": "KFUG2ieQXG0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"My dog has died of cancer yesterday. Do you think it is possible to revive him bck using alchemy.\""
      ],
      "metadata": {
        "id": "NzkTGoaqXush"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=word_tokenize(text)\n",
        "tokens=[a.lower() for a in tokens]\n",
        "stripped_tokens=[a for a in tokens if a not in string.punctuation]"
      ],
      "metadata": {
        "id": "-5QKXSwmX_wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stripped_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6z8cvi3ZSJQ",
        "outputId": "2ef0f1ef-34cc-48c4-cb95-a6d635387e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my',\n",
              " 'dog',\n",
              " 'has',\n",
              " 'died',\n",
              " 'of',\n",
              " 'cancer',\n",
              " 'yesterday',\n",
              " 'do',\n",
              " 'you',\n",
              " 'think',\n",
              " 'it',\n",
              " 'is',\n",
              " 'possible',\n",
              " 'to',\n",
              " 'revive',\n",
              " 'him',\n",
              " 'bck',\n",
              " 'using',\n",
              " 'alchemy']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2mt1pRmZDdU",
        "outputId": "b3eb19f5-d111-45e1-ae4f-3806a91ffaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my',\n",
              " 'dog',\n",
              " 'has',\n",
              " 'died',\n",
              " 'of',\n",
              " 'cancer',\n",
              " 'yesterday',\n",
              " '.',\n",
              " 'do',\n",
              " 'you',\n",
              " 'think',\n",
              " 'it',\n",
              " 'is',\n",
              " 'possible',\n",
              " 'to',\n",
              " 'revive',\n",
              " 'him',\n",
              " 'bck',\n",
              " 'using',\n",
              " 'alchemy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_stopwords=[a for a in stripped_tokens if a not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "lW1wGFa8ZbQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT13yUHnZmHb",
        "outputId": "f4e89f4f-8c47-473c-df13-11ba5d2051e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog',\n",
              " 'died',\n",
              " 'cancer',\n",
              " 'yesterday',\n",
              " 'think',\n",
              " 'possible',\n",
              " 'revive',\n",
              " 'bck',\n",
              " 'using',\n",
              " 'alchemy']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vocabulary Matching"
      ],
      "metadata": {
        "id": "2-kLm-MFZtji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`matcher` is used from spacy library to match a pattern from your given strings and give out words connected to the patterns or matches.\n",
        "(Basically helps to make words having two different words to make one like solarpower easier for the library to read by giving it patterns related to how it can phrased(like solarpower or solar power))"
      ],
      "metadata": {
        "id": "ACGzP0u4MsfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "B7dypUHfZtSQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher=Matcher(nlp.vocab)\n"
      ],
      "metadata": {
        "id": "6q61NdNMOu6N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1=[{'LOWER':'solarpower'}]\n",
        "pattern2=[{'LOWER':'solar'},{'LOWER':'power'}]\n",
        "pattern3=[{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]"
      ],
      "metadata": {
        "id": "_-soNfLmPYb4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern=[pattern1,pattern2,pattern3]"
      ],
      "metadata": {
        "id": "pwU-UMtUQ5t5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher.add(\"SolarPower\",pattern)"
      ],
      "metadata": {
        "id": "2GxaO8GCPfxu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Aaj maine boht jyada solar power use krliya warna solarpower ko solar-power bhi bolskte the na?\""
      ],
      "metadata": {
        "id": "lLu18T8rPw5j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(text)\n",
        "found_matches=matcher(doc)"
      ],
      "metadata": {
        "id": "VmTvxfjNSLn_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "found_matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu0ozuttSLl1",
        "outputId": "9ec5611a-6d28-4328-f984-cb1a418246cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8656102463236116519, 4, 6),\n",
              " (8656102463236116519, 9, 10),\n",
              " (8656102463236116519, 11, 14)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can also use `{LEMMA:'word'}` to make it easier for lemmatization and pattern matching together"
      ],
      "metadata": {
        "id": "ATN6XuOIS_tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phrase Matcher"
      ],
      "metadata": {
        "id": "3lMT3yrDTbZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "YNpzYvZ3SLjW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher=PhraseMatcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "naByk-ODSLf9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "works similar to matcher"
      ],
      "metadata": {
        "id": "yiXNAuXtUHFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phone=[\"Samsung Galaxy\", \"Iphone 15\", \"Oppo f15\"]\n",
        "patterns=[nlp(text) for text in phone]\n",
        "matcher.add(\"Phone\",patterns)"
      ],
      "metadata": {
        "id": "LuzeehtzSLb0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_text=\"Mere pe hai Iphone 15 aur mere dost pe hai Samsung Galaxy aur mere dusre dost pe hai Oppo f15. Isme se sabse ameer oppo wala hai.\""
      ],
      "metadata": {
        "id": "pPLz7FAnSLUw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches=matcher(nlp(doc_text))\n",
        "matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lizelKO8VKQy",
        "outputId": "fac036da-c95d-4a3d-e586-62bdb887ed9c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9908896510488767061, 3, 5),\n",
              " (9908896510488767061, 10, 12),\n",
              " (9908896510488767061, 18, 20)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "TrDfCGWrX8FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I think you work in Amazon and Microsoft at the same time.\""
      ],
      "metadata": {
        "id": "Q85bhPYoYCuf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n"
      ],
      "metadata": {
        "id": "7--lvSAxYrJo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(text)\n",
        "entity=[]\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,ent.label_)\n",
        "    entity.append(ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecm31MMlYwSC",
        "outputId": "f1e5a2ed-09fe-46d8-94bf-8979674b438a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon ORG\n",
            "Microsoft ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jQEy9xqY-hW",
        "outputId": "bc406900-4507-4246-de6e-376d2c07118e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Amazon', 'Microsoft']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Senetnce Segmentation"
      ],
      "metadata": {
        "id": "yaf7_u9oZ98S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "text=\"This is the first sentence. This is the second sentence. This is the third sentence.\"\n",
        "doc=nlp(text)\n",
        "sentence=[]\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        "    sentence.append(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1K6npptaG2y",
        "outputId": "bfb2e53d-adde-4d71-fe52-61afe685fcbb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "This is the second sentence.\n",
            "This is the third sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-ymf-OaVoE",
        "outputId": "305dc3b0-0ce5-43e3-8b10-36c071e0f5e3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is the first sentence.,\n",
              " This is the second sentence.,\n",
              " This is the third sentence.]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Keras for data pereprocessing"
      ],
      "metadata": {
        "id": "3CNlg5TMbIzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAYPnBL0z-1r",
        "outputId": "05a8c474-8d75-49e4-c69b-eb4005ccefdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras_preprocessing.text import Tokenizer,text_to_word_sequence"
      ],
      "metadata": {
        "id": "Mogxici_zuPF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I would like to lay eggs on the nest of the nearesr eagle bird so that they understand how big-they are and what they look like.\""
      ],
      "metadata": {
        "id": "zJrhbyDN0E7d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=text_to_word_sequence(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtaN7trc0O1u",
        "outputId": "a69094b5-91f0-482b-d42e-356d318a7ad1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'would',\n",
              " 'like',\n",
              " 'to',\n",
              " 'lay',\n",
              " 'eggs',\n",
              " 'on',\n",
              " 'the',\n",
              " 'nest',\n",
              " 'of',\n",
              " 'the',\n",
              " 'nearesr',\n",
              " 'eagle',\n",
              " 'bird',\n",
              " 'so',\n",
              " 'that',\n",
              " 'they',\n",
              " 'understand',\n",
              " 'how',\n",
              " 'big',\n",
              " 'they',\n",
              " 'are',\n",
              " 'and',\n",
              " 'what',\n",
              " 'they',\n",
              " 'look',\n",
              " 'like']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.text import hashing_trick"
      ],
      "metadata": {
        "id": "pH_cXMjg0Zd2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using hashing reduces computation (mainly to be used for small data projects)"
      ],
      "metadata": {
        "id": "bY8nskCq6gZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=set(result)\n",
        "vocab=len(words)\n",
        "result1=hashing_trick(text,vocab,hash_function='md5')\n",
        "result1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGo1HriS6Dgx",
        "outputId": "3a69d512-1359-42f6-921a-98e42144442a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20,\n",
              " 19,\n",
              " 16,\n",
              " 16,\n",
              " 7,\n",
              " 4,\n",
              " 17,\n",
              " 18,\n",
              " 13,\n",
              " 9,\n",
              " 18,\n",
              " 21,\n",
              " 15,\n",
              " 8,\n",
              " 7,\n",
              " 10,\n",
              " 21,\n",
              " 15,\n",
              " 18,\n",
              " 7,\n",
              " 21,\n",
              " 2,\n",
              " 3,\n",
              " 19,\n",
              " 21,\n",
              " 13,\n",
              " 16]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer"
      ],
      "metadata": {
        "id": "XzBFjpI98ciS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "EkO5zndQ5_yS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVNc1nDh8jIl",
        "outputId": "e5460127-d746-428a-f3a0-de4afb7ff6d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('i', 1), ('would', 1), ('like', 2), ('to', 1), ('lay', 1), ('eggs', 1), ('on', 1), ('the', 2), ('nest', 1), ('of', 1), ('nearesr', 1), ('eagle', 1), ('bird', 1), ('so', 1), ('that', 1), ('they', 3), ('understand', 1), ('how', 1), ('big', 1), ('are', 1), ('and', 1), ('what', 1), ('look', 1)])\n"
          ]
        }
      ]
    }
  ]
}